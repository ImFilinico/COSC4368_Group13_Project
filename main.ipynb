{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e29f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Header and Imports\n",
    "\"\"\"\n",
    "DenseNet-121 Baseline Model for NIH Chest X-ray Classification\n",
    "COSC 4368 Final Project - Group 13\n",
    "Yla Herrera, Nicolas Mangilit, Kiriti Padavala, and Matt Tindall\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Seeds and Check Device\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# heck for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Paths and Load Metadata\n",
    "DATA_DIR = \"data\" \n",
    "\n",
    "# The dataset has images split across multiple folders: images_001 to images_012\n",
    "\n",
    "#Load metadata\n",
    "print(\"Loading metadata CSV...\")\n",
    "csv_path = os.path.join(DATA_DIR, \"Data_Entry_2017.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"✓ Total number of images: {len(df)}\")\n",
    "print(f\"\\nDataset columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "#Look at image folders\n",
    "print(f\"\\nChecking for image folders...\")\n",
    "image_folders = []\n",
    "for i in range(1, 13):  #all folders\n",
    "    folder_name = f\"images_{i:03d}\"\n",
    "    folder_path = os.path.join(DATA_DIR, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        num_images = len([f for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "        image_folders.append(folder_name)\n",
    "        print(f\"Found {folder_name} with {num_images} images\")\n",
    "\n",
    "print(f\"\\nTotal image folders found: {len(image_folders)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Disease Categories\n",
    "# The 14 disease categories in NIH Chest X-ray dataset\n",
    "DISEASE_CATEGORIES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', \n",
    "    'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', \n",
    "    'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', \n",
    "    'Pleural_Thickening', 'Hernia'\n",
    "]\n",
    "\n",
    "print(f\"Number of disease categories: {len(DISEASE_CATEGORIES)}\")\n",
    "print(f\"Disease categories: {DISEASE_CATEGORIES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Labels and Create Binary Columns\n",
    "# Parse labels (Finding Labels column contains multiple labels separated by |)\n",
    "def parse_labels(label_string, disease_categories):\n",
    "    \"\"\"Convert label string to multi-hot encoded vector\"\"\"\n",
    "    labels = np.zeros(len(disease_categories))\n",
    "    if label_string != \"No Finding\":\n",
    "        diseases = label_string.split('|')\n",
    "        for disease in diseases:\n",
    "            if disease in disease_categories:\n",
    "                idx = disease_categories.index(disease)\n",
    "                labels[idx] = 1\n",
    "    return labels\n",
    "\n",
    "# Add binary labels for each disease\n",
    "for disease in DISEASE_CATEGORIES:\n",
    "    df[disease] = df['Finding Labels'].apply(\n",
    "        lambda x: 1 if disease in x else 0\n",
    "    )\n",
    "\n",
    "print(f\"\\nDisease distribution:\")\n",
    "print(df[DISEASE_CATEGORIES].sum().sort_values(ascending=False))\n",
    "\n",
    "# Visualize disease distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "disease_counts = df[DISEASE_CATEGORIES].sum().sort_values(ascending=False)\n",
    "plt.bar(range(len(disease_counts)), disease_counts.values)\n",
    "plt.xticks(range(len(disease_counts)), disease_counts.index, rotation=45, ha='right')\n",
    "plt.xlabel('Disease Category')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.title('Distribution of Diseases in Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f1de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for NIH Chest X-rays\n",
    "\n",
    "    image_dir may be a single path (string) or a list/tuple of directories.\n",
    "    If multiple directories are provided, __getitem__ will search them in order\n",
    "    for the requested image filename.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, image_dir, disease_categories, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        # allow a single path or multiple roots\n",
    "        if isinstance(image_dir, (list, tuple)):\n",
    "            self.image_dirs = list(image_dir)\n",
    "        else:\n",
    "            self.image_dirs = [image_dir]\n",
    "\n",
    "        # keep original for compatibility\n",
    "        self.image_dir = image_dir\n",
    "        self.disease_categories = disease_categories\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image filename\n",
    "        img_name = self.dataframe.loc[idx, 'Image Index']\n",
    "\n",
    "        # Try each candidate root until file is found\n",
    "        img_path = None\n",
    "        for root in self.image_dirs:\n",
    "            candidate = os.path.join(root, img_name)\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "\n",
    "        if img_path is None:\n",
    "            # helpful message showing where we looked\n",
    "            raise FileNotFoundError(f\"Image '{img_name}' not found in any of: {self.image_dirs}\")\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Get labels (multi-hot encoded)\n",
    "        labels = self.dataframe.loc[idx, self.disease_categories].values.astype(np.float32)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Define Data Transforms\n",
    "# Define transforms for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Data transforms defined:\")\n",
    "print(f\"  - Train: Resize(224x224), RandomHorizontalFlip, ToTensor, Normalize\")\n",
    "print(f\"  - Val: Resize(224x224), ToTensor, Normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2085433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train/Validation Split\n",
    "# Simple train/validation split (80/20)\n",
    "# For your project, you may want to use the official NIH train/val/test split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df = train_df.sample(n=8000, random_state=42)\n",
    "val_df = val_df.sample(n=2000, random_state=42)\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"\\nTrain/Val split ratio: {len(train_df)/len(df):.2%} / {len(val_df)/len(df):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce89b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create Datasets and DataLoaders\n",
    "# Create datasets\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# The images are split across multiple folders images_001..images_012 (each contains an inner 'images' folder).\n",
    "# Build a list of directories to search for files and pass that to the dataset so it can locate each image.\n",
    "IMAGE_DIRS = [os.path.join(DATA_DIR, f\"images_{i:03d}\", \"images\") for i in range(1, 13)]\n",
    "# keep only existing directories (helps if some folders are missing locally)\n",
    "IMAGE_DIRS = [p for p in IMAGE_DIRS if os.path.exists(p)]\n",
    "print(f\"Found {len(IMAGE_DIRS)} image directories; using:\\n  - \" + \"\\n  - \".join(IMAGE_DIRS))\n",
    "\n",
    "train_dataset = ChestXrayDataset(\n",
    "    train_df, \n",
    "    IMAGE_DIRS, \n",
    "    DISEASE_CATEGORIES, \n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDataset(\n",
    "    val_df, \n",
    "    IMAGE_DIRS, \n",
    "    DISEASE_CATEGORIES, \n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "# Use a smaller batch size on CPU and avoid worker processes on Windows (spawn overhead can be very slow)\n",
    "BATCH_SIZE = 8  # Reduce for CPU (increase if you have more RAM/CPU)\n",
    "if os.name == 'nt':\n",
    "    NUM_WORKERS = 0\n",
    "else:\n",
    "    NUM_WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")\n",
    "print(f\"  - Num workers: {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eeecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic & Quick Debug Training (CPU-friendly)\n",
    "# Measures DataLoader load/transfer times and runs a short 1-epoch training on a small subset\n",
    "import time\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "print('Device:', device)\n",
    "print('Train dataset length:', len(train_dataset))\n",
    "print('Train loader batches:', len(train_loader))\n",
    "print('Batch size:', BATCH_SIZE, 'Num workers:', NUM_WORKERS)\n",
    "\n",
    "# Ensure a model is available on CPU for the debug run\n",
    "if 'model' not in globals():\n",
    "    print(\"`model` not found — initializing DenseNet121 (pretrained=False) for debug.\")\n",
    "    try:\n",
    "        model = DenseNet121(num_classes=len(DISEASE_CATEGORIES), pretrained=False)\n",
    "    except NameError:\n",
    "        print(\"`DenseNet121` class not found — building model directly from torchvision.models\")\n",
    "        base = models.densenet121(pretrained=False)\n",
    "        num_features = base.classifier.in_features\n",
    "        base.classifier = nn.Linear(num_features, len(DISEASE_CATEGORIES))\n",
    "        model = base\n",
    "\n",
    "# Move model to CPU for this debug run\n",
    "model = model.to('cpu')\n",
    "\n",
    "# Ensure a loss function exists\n",
    "if 'criterion' not in globals():\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use a small subset for a quick smoke test\n",
    "DEBUG_SAMPLES = min(200, len(train_dataset))\n",
    "subset_idx = list(range(DEBUG_SAMPLES))\n",
    "debug_loader = DataLoader(Subset(train_dataset, subset_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Measure first few batches (load + forward+loss computation)\n",
    "load_times = []\n",
    "step_times = []\n",
    "\n",
    "it = iter(debug_loader)\n",
    "for i in range(min(5, len(debug_loader))):\n",
    "    t0 = time.time()\n",
    "    imgs, lbls = next(it)\n",
    "    t1 = time.time()\n",
    "    load_times.append(t1 - t0)\n",
    "\n",
    "    t2 = time.time()\n",
    "    # ensure tensors on CPU (this notebook runs on CPU)\n",
    "    outputs = model(imgs)\n",
    "    loss = criterion(outputs, lbls)\n",
    "    t3 = time.time()\n",
    "    step_times.append(t3 - t2)\n",
    "    print(f\"Batch {i+1}: load={load_times[-1]:.3f}s, forward+loss={step_times[-1]:.3f}s\")\n",
    "\n",
    "if load_times:\n",
    "    print(f\"Avg load time: {sum(load_times)/len(load_times):.3f}s, avg forward+loss: {sum(step_times)/len(step_times):.3f}s\")\n",
    "\n",
    "# Quick 1-epoch training on the small subset (use CPU)\n",
    "print('\\nStarting short debug training (1 epoch over {} samples)'.format(DEBUG_SAMPLES))\n",
    "optimizer_debug = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "start = time.time()\n",
    "for i, (imgs, lbls) in enumerate(debug_loader, 1):\n",
    "    t0 = time.time()\n",
    "    optimizer_debug.zero_grad()\n",
    "    outputs = model(imgs)\n",
    "    loss = criterion(outputs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer_debug.step()\n",
    "    t1 = time.time()\n",
    "    if i % 10 == 0 or i == len(debug_loader):\n",
    "        print(f\"Step {i}/{len(debug_loader)} - loss={loss.item():.4f} step_time={t1-t0:.3f}s\")\n",
    "end = time.time()\n",
    "print(f\"Debug epoch complete in {end - start:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke-check: verify datasets & that an example image file exists\n",
    "print('IMAGE_DIRS:', IMAGE_DIRS)\n",
    "print('Train dataset length:', len(train_dataset))\n",
    "print('Val dataset length:', len(val_dataset))\n",
    "\n",
    "# Small helper to locate a filename inside the configured roots\n",
    "from pathlib import Path\n",
    "\n",
    "def find_image_path(image_name, search_dirs):\n",
    "    for d in search_dirs:\n",
    "        candidate = os.path.join(d, image_name)\n",
    "        if os.path.exists(candidate):\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "sample_name = train_df.iloc[0]['Image Index']\n",
    "print('Sample image name (from train_df):', sample_name)\n",
    "found = find_image_path(sample_name, IMAGE_DIRS)\n",
    "print('Found sample path:' , found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f91ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 10: Test DataLoader (Optional - verify it works)\n",
    "# # Test loading a batch\n",
    "# print(\"Testing data loading...\")\n",
    "# images, labels = next(iter(train_loader))\n",
    "# print(f\"Batch shape: {images.shape}\")\n",
    "# print(f\"Labels shape: {labels.shape}\")\n",
    "# print(f\"Image value range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "\n",
    "# # Visualize a few sample images\n",
    "# fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# for i in range(8):\n",
    "#     img = images[i].permute(1, 2, 0).numpy()\n",
    "#     # Denormalize\n",
    "#     img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "#     img = np.clip(img, 0, 1)\n",
    "    \n",
    "#     axes[i].imshow(img)\n",
    "#     # Get disease labels for this image\n",
    "#     disease_indices = torch.where(labels[i] == 1)[0]\n",
    "#     disease_names = [DISEASE_CATEGORIES[idx] for idx in disease_indices]\n",
    "#     title = ', '.join(disease_names) if disease_names else 'No Finding'\n",
    "#     axes[i].set_title(title, fontsize=9)\n",
    "#     axes[i].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# print(\"Data loading test successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Define DenseNet-121 Model\n",
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"DenseNet-121 for multi-label chest X-ray classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=14, pretrained=True):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        \n",
    "        # Load pre-trained DenseNet-121\n",
    "        self.densenet = models.densenet121(pretrained=pretrained)\n",
    "        \n",
    "        # Get number of input features for the classifier\n",
    "        num_features = self.densenet.classifier.in_features\n",
    "        \n",
    "        # Replace the classifier for multi-label classification\n",
    "        self.densenet.classifier = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.densenet(x)\n",
    "\n",
    "print(\"DenseNet121 model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60318a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 12: Initialize Model\n",
    "# Initialize model\n",
    "model = DenseNet121(num_classes=len(DISEASE_CATEGORIES), pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model initialized and moved to {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a353a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Define Loss Function and Optimizer\n",
    "# Binary Cross Entropy with Logits Loss (suitable for multi-label classification)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "LEARNING_RATE = 1e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "print(f\"Loss function: BCEWithLogitsLoss\")\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Training Function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32523cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Validation Function\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    \n",
    "    # Calculate AUC for each disease\n",
    "    auc_scores = []\n",
    "    for i in range(all_labels.shape[1]):\n",
    "        if len(np.unique(all_labels[:, i])) > 1:  # Check if both classes present\n",
    "            auc = roc_auc_score(all_labels[:, i], all_preds[:, i])\n",
    "            auc_scores.append(auc)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)\n",
    "    \n",
    "    mean_auc = np.nanmean(auc_scores)\n",
    "    \n",
    "    return epoch_loss, mean_auc, auc_scores\n",
    "\n",
    "print(\"Validation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Training Loop\n",
    "NUM_EPOCHS = 5  # Start with a few epochs to test\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Training - {NUM_EPOCHS} epochs\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_aucs = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, mean_auc, auc_scores = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_aucs.append(mean_auc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Mean AUC: {mean_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-disease AUC scores:\")\n",
    "    for disease, auc in zip(DISEASE_CATEGORIES, auc_scores):\n",
    "        if not np.isnan(auc):\n",
    "            print(f\"  {disease:25s}: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Save Model\n",
    "# Save the trained model\n",
    "MODEL_SAVE_PATH = \"densenet121_baseline.pth\"\n",
    "torch.save({\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_aucs': val_aucs,\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Cell 18: Plot Training Curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), val_losses, marker='s', label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# AUC curve\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, NUM_EPOCHS+1), val_aucs, marker='o', color='green', label='Mean AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Validation Mean AUC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Summary table\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.axis('off')\n",
    "summary_data = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    summary_data.append([i+1, f\"{train_losses[i]:.4f}\", f\"{val_losses[i]:.4f}\", f\"{val_aucs[i]:.4f}\"])\n",
    "\n",
    "table = plt.table(cellText=summary_data,\n",
    "                  colLabels=['Epoch', 'Train Loss', 'Val Loss', 'Mean AUC'],\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "plt.title('Training Summary', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved to 'training_curves.png'\")\n",
    "\n",
    "# Cell 19: Final Results Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Validation AUC: {max(val_aucs):.4f} (Epoch {val_aucs.index(max(val_aucs))+1})\")\n",
    "print(f\"Final Validation AUC: {val_aucs[-1]:.4f}\")\n",
    "print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Baseline model training complete!\")\n",
    "print(\"Next steps: Implement bias mitigation techniques\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
